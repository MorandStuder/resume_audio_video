import os
import tkinter as tk
from tkinter import filedialog
import json
from pathlib import Path
from openai import OpenAI
# Imports lourds (whisper, split_audio) d√©plac√©s dans les fonctions pour acc√©l√©rer le d√©marrage


def get_media_path():
    """Ouvre une fen√™tre de s√©lection de fichier vid√©o ou audio"""
    root = tk.Tk()
    root.withdraw()
    root.attributes('-topmost', True)  # Garde la fen√™tre au premier plan
    root.lift()  # Force la fen√™tre au premier plan
    root.focus_force()  # Force le focus
    root.update()  # Force le rendu pour s'assurer que tout est pr√™t

    file_path = filedialog.askopenfilename(
        title="S√©lectionnez votre fichier vid√©o ou audio",
        filetypes=[
            ("Fichiers audio", "*.mp3 *.wav *.ogg *.flac"),
            ("Fichiers vid√©o", "*.mp4 *.avi *.mkv *.mov"),
            ("Tous les fichiers", "*.*"),
        ],
        # initialdir=os.path.expanduser("~")
        initialdir="C:/Users/moran/OneDrive/Documents/Audacity",
        parent=root,  # Sp√©cifie le parent pour h√©riter des attributs
    )

    root.destroy()  # Ferme proprement la fen√™tre apr√®s utilisation

    if file_path and os.path.exists(file_path):
        return file_path
        print(file_path)
    return None


def split_audio_legacy(input_path, segment_duration_min=30):
    """Ancienne fonction, redirig√©e vers le module commun split_audio."""
    # Import diff√©r√© pour acc√©l√©rer le d√©marrage
    from Split import split_audio
    return split_audio(input_path, segment_duration_min, output_directory="segments_audio")


def transcribe_segments(segments_paths):
    """Transcrit les segments audio avec Whisper"""
    print("\n=== √âTAPE 2: TRANSCRIPTION ===")
    
    # Import diff√©r√© pour acc√©l√©rer le d√©marrage
    import warnings
    # Supprimer l'avertissement FP16 sur CPU (Whisper utilise automatiquement FP32)
    warnings.filterwarnings("ignore", message="FP16 is not supported on CPU; using FP32 instead")
    
    import whisper
    from tqdm import tqdm

    output_directory = "transcriptions"
    os.makedirs(output_directory, exist_ok=True)

    print("Chargement du mod√®le Whisper...")
    model = whisper.load_model("base")
    transcriptions = []

    # Barre de progression pour la transcription
    with tqdm(total=len(segments_paths), desc="Transcription", unit="segment") as pbar:
        for i, audio_path in enumerate(segments_paths, 1):
            pbar.set_description(f"Transcription du segment {i}/{len(segments_paths)}")
            result = model.transcribe(audio_path, language="fr")

            # Sauvegarder la transcription individuelle
            output_path = os.path.join(output_directory, f"transcription_{i:02d}.txt")
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(result["text"])

            transcriptions.append(result["text"])
            pbar.update(1)

    # Sauvegarder toutes les transcriptions dans un seul fichier
    full_transcript_path = os.path.join(output_directory, "transcription_complete.txt")
    with open(full_transcript_path, "w", encoding="utf-8") as f:
        for i, text in enumerate(transcriptions, 1):
            f.write(f"\n{'='*50}\n")
            f.write(f"SEGMENT {i}\n")
            f.write(f"{'='*50}\n\n")
            f.write(text)
            f.write("\n\n")

    print(f"‚úÖ Transcription compl√®te sauvegard√©e: {full_transcript_path}")

    return transcriptions


def load_api_key():
    """Charge la cl√© API depuis le fichier config ou la demande √† l'utilisateur"""
    config_file = Path("config.json")

    if config_file.exists():
        with open(config_file, "r") as f:
            config = json.load(f)
            api_key = config.get("OPENAI_API_KEY", "")
            if api_key:
                return api_key

    # Si pas de cl√©, demander √† l'utilisateur
    api_key = input("Entrez votre cl√© API OpenAI: ").strip()

    # Sauvegarder la cl√© pour la prochaine fois
    with open(config_file, "w") as f:
        json.dump({"OPENAI_API_KEY": api_key}, f)

    return api_key


def summarize_transcriptions(transcriptions):
    """R√©sume les transcriptions avec GPT"""
    print("\n=== √âTAPE 3: R√âSUM√â ===")
    
    from tqdm import tqdm

    # Charger la cl√© API
    client = OpenAI(api_key=load_api_key())

    output_directory = "resumes"
    os.makedirs(output_directory, exist_ok=True)

    # Concat√©ner toutes les transcriptions
    full_text = "\n\n".join(transcriptions)

    # Barre de progression pour la g√©n√©ration du r√©sum√©
    with tqdm(desc="G√©n√©ration du r√©sum√© avec GPT", unit="op√©ration") as pbar:
        pbar.set_description("Cr√©ation du r√©sum√© global...")
        response = client.chat.completions.create(
            model="gpt-5",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "Tu es un assistant qui met en forme des transcriptions "
                        "de vid√©os ou de r√©unions."
                    ),
                },
                {
                    "role": "user",
                    "content": (
                        "Met en forme cette transcription en fran√ßais en gardant "
                        "le maximum d'informations, sous la forme d'un texte "
                        "structur√©, avec des titres et des sous-titres. "
                        "Organise le contenu de mani√®re claire et logique, "
                        "en mettant en √©vidence les points importants et les "
                        "√©ventuelles d√©cisions prises.\n\n"
                        f"{full_text}"
                    ),
                },
            ],
        )
        pbar.update(1)
        pbar.set_description("R√©cup√©ration de la r√©ponse...")
        global_summary = response.choices[0].message.content
        pbar.update(1)

    # Sauvegarder le r√©sum√© global
    with open(
        os.path.join(output_directory, "resume_global.txt"), "w", encoding="utf-8"
    ) as f:
        f.write(global_summary)

    print(f"‚úÖ R√©sum√© global sauvegard√© dans: " f"{os.path.abspath(output_directory)}")

    return global_summary


def main():
    print("=== TRAITEMENT M√âDIA ===")

    # √âtape 1: S√©lection et d√©coupage
    print("\nS√©lectionnez votre fichier vid√©o ou audio...")
    input_path = get_media_path()
    if not input_path:
        print("‚ùå Aucun fichier s√©lectionn√©.")
        return

    try:
        # D√©coupage audio
        print("\n=== √âTAPE 1: D√âCOUPAGE ===")
        segments_paths = split_audio_legacy(input_path)

        # Transcription
        transcriptions = transcribe_segments(segments_paths)

        # R√©sum√©
        summarize_transcriptions(transcriptions)

        print("\n‚úÖ Traitement termin√© avec succ√®s!")
        print("üìÇ Vous trouverez les fichiers dans les dossiers:")
        print(f"   - Audio: {os.path.abspath('segments_audio')}")
        print(f"   - Transcriptions: {os.path.abspath('transcriptions')}")
        print(f"   - R√©sum√©s: {os.path.abspath('resumes')}")

    except Exception as e:
        print(f"\n‚ùå Une erreur s'est produite: {str(e)}")

    input("\nAppuyez sur Entr√©e pour fermer...")


if __name__ == "__main__":
    main()
